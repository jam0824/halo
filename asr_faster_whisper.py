import queue
import sys
import time
import wave
from dataclasses import dataclass

import numpy as np
import sounddevice as sd
import webrtcvad
from faster_whisper import WhisperModel


@dataclass
class Config:
    samplerate: int = 16000      # webrtcvad ã¯ 8/16/32/48k ã®ã¿å¯¾å¿œ
    channels: int = 1            # ãƒ¢ãƒãƒ©ãƒ«
    frame_ms: int = 30           # 10/20/30ms ã®ã„ãšã‚Œã‹
    vad_aggressiveness: int = 2  # 0(ç”˜ã„) - 3(å³ã—ã„)
    start_trigger_ms: int = 200  # é€£ç¶šç™ºè©±ã¨ã¿ãªã™ã¾ã§ï¼ˆé–‹å§‹ï¼‰: 200ms
    end_trigger_ms: int = 500    # ç„¡éŸ³ãŒç¶šã„ãŸã‚‰åœæ­¢ï¼ˆçµ‚äº†ï¼‰: 800ms
    max_record_sec: int = 60     # å®‰å…¨ã®ãŸã‚ã®ä¸Šé™
    output_wav: str = "capture.wav"
    model_name: str = "tiny"     # Pi ãªã‚‰ tiny æ¨å¥¨ï¼ˆå¿…è¦ãªã‚‰ base ã¸ï¼‰
    language: str | None = "ja"  # è‡ªå‹•æ¤œå‡ºã«ä»»ã›ã‚‹ãªã‚‰ None


def write_wave(path: str, audio_bytes: bytes, cfg: Config):
    with wave.open(path, "wb") as wf:
        wf.setnchannels(cfg.channels)
        wf.setsampwidth(2)  # int16
        wf.setframerate(cfg.samplerate)
        wf.writeframes(audio_bytes)


def main():
    cfg = Config()
    vad = webrtcvad.Vad(cfg.vad_aggressiveness)

    # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãƒ»ãƒã‚¤ãƒˆæ•°
    samples_per_frame = int(cfg.samplerate * cfg.frame_ms / 1000)  # 16000 * 0.03 = 480
    bytes_per_frame = samples_per_frame * 2  # int16

    q_in = queue.Queue()

    def audio_callback(indata, frames, time_info, status):
        if status:
            # XRuns ãªã©ã®è­¦å‘Šã¯æ¨™æº–ã‚¨ãƒ©ãƒ¼ã¸
            print(status, file=sys.stderr)
        # float32 -> int16 ã¸å¤‰æ›ï¼ˆã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°æ³¨æ„ï¼‰
        pcm16 = np.clip(indata[:, 0], -1.0, 1.0)
        pcm16 = (pcm16 * 32767.0).astype(np.int16).tobytes()
        # 30ms ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã«åˆ‡ã‚‹ï¼ˆä½™ã‚Šã¯æ¬¡å›ã«å›ã™ï¼‰
        audio_buffer = audio_callback.buffer + pcm16
        # ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã§ã‚­ãƒ¥ãƒ¼ã¸
        offset = 0
        while offset + bytes_per_frame <= len(audio_buffer):
            q_in.put(audio_buffer[offset: offset + bytes_per_frame])
            offset += bytes_per_frame
        # ä½™ã‚Šã‚’ãƒãƒƒãƒ•ã‚¡ã«æˆ»ã™
        audio_callback.buffer = audio_buffer[offset:]

    audio_callback.buffer = b""

    # éŒ²éŸ³é–‹å§‹
    print("ğŸ™ï¸ è©±ã—å§‹ã‚ã¦ãã ã•ã„ï¼ˆç„¡éŸ³ãŒç¶šãã¨è‡ªå‹•åœæ­¢ã—ã¾ã™ï¼‰...")
    stream = sd.InputStream(
        samplerate=cfg.samplerate,
        channels=cfg.channels,
        dtype="float32",
        blocksize=samples_per_frame,  # 1ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã§ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        callback=audio_callback,
        device=None,  # æ—¢å®šãƒ‡ãƒã‚¤ã‚¹ã€‚å¿…è¦ãªã‚‰ index ã‚„ name ã§æŒ‡å®š
    )

    started = False
    voiced_ms = 0
    silence_ms = 0
    collected = []

    t0 = time.time()
    with stream:
        while True:
            try:
                frame = q_in.get(timeout=0.5)
            except queue.Empty:
                # å…¥åŠ›ãŒæ¥ã¦ã„ãªã„ã€‚æœ€å¤§éŒ²éŸ³ç§’æ•°ã§æ‰“ã¡åˆ‡ã‚Š
                if time.time() - t0 > cfg.max_record_sec:
                    break
                continue

            is_speech = vad.is_speech(frame, cfg.samplerate)

            if is_speech:
                voiced_ms += cfg.frame_ms
                silence_ms = 0
            else:
                silence_ms += cfg.frame_ms

            # é–‹å§‹åˆ¤å®šï¼šä¸€å®šä»¥ä¸Šã®ç™ºè©±ãŒç¶šã„ãŸã‚‰ã€ŒéŒ²éŸ³é–‹å§‹çŠ¶æ…‹ã€ã¸
            if not started and voiced_ms >= cfg.start_trigger_ms:
                started = True
                # ç›´å‰ã«æºœã‚ãŸåˆ†ã‚‚å«ã‚ã¦åé›†é–‹å§‹
                # ï¼ˆç°¡æ˜“åŒ–ã®ãŸã‚ã€ã“ã“ã§ã¯ãƒ•ãƒ¬ãƒ¼ãƒ åˆ°ç€ä»¥é™ã ã‘ã‚’ä¿å­˜ï¼‰
                # ã‚ˆã‚Šå³å¯†ã«ã—ãŸã‘ã‚Œã°ãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ã§é¡ã£ã¦ä¿å­˜ã™ã‚‹
            if started:
                collected.append(frame)

            # çµ‚äº†åˆ¤å®šï¼šé–‹å§‹å¾Œã«ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶šã„ãŸã‚‰åœæ­¢
            if started and silence_ms >= cfg.end_trigger_ms:
                break

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šé•·ã™ãã‚‹éŒ²éŸ³ã¯å¼·åˆ¶åœæ­¢
            if time.time() - t0 > cfg.max_record_sec:
                break

    if not collected:
        print("éŸ³å£°ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        return

    audio_bytes = b"".join(collected)
    write_wave(cfg.output_wav, audio_bytes, cfg)
    print(f"ğŸ’¾ ä¿å­˜: {cfg.output_wav} ({len(collected)} frames)")

    # ã™ãã«æ–‡å­—èµ·ã“ã—
    print("ğŸ“ æ–‡å­—èµ·ã“ã—ä¸­...")
    start_time = time.perf_counter()
    model = WhisperModel(cfg.model_name, device="cpu", compute_type="int8")
    segments, info = model.transcribe(cfg.output_wav, language=cfg.language, beam_size=1)
    print(f"Detected: {info.language} ({info.language_probability:.2f})")
    for seg in segments:
        print(f"[{seg.start:.2f} -> {seg.end:.2f}] {seg.text}")
    end_time = time.perf_counter()
    print(f"[ASR latency] {end_time - start_time:.1f} s")


if __name__ == "__main__":
    main()
